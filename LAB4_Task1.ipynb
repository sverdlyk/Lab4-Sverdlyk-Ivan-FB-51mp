{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 4, Task 1  Sverdlyk Ivan, FB-51mp.**\n",
        "В даній частині лабораторної роботи реалізовано машинний переклад з англійської на українську мову. Використано архітектуру Seq2Seq на базі RNN (GRU), з механізмом уваги."
      ],
      "metadata": {
        "id": "9tuaPRygHYfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вимушений був скористатись Colab, адже навчання відбувається дуже довго на CPU, а хотілось би побачити результат.\n",
        "# Розпочав з підключення всього необхідного\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "# (GPU або CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Пристрій для навчання: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reS--NMeH6tn",
        "outputId": "38510331-42bb-4cb5-b12f-3b8bee7f5dd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пристрій для навчання: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токени\n",
        "SOS_token = 0  # Start Of Sentence (Початок)\n",
        "EOS_token = 1  # End Of Sentence (Кінець)\n",
        "UNK_token = 2  # Unknown (Невідоме слово)\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        # Ініціалізація з трьома токенами\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 3\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "euekpgw7IjSi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Очищення та нормалізація тексту\n",
        "def normalizeString(s):\n",
        "    # Нижній регістр, видалення пробілів\n",
        "    s = s.lower().strip()\n",
        "    # Відокремлюємо пунктуацію від слів\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # Залишаємо тільки літери (латиниця + кирилиця) та основні знаки\n",
        "    s = re.sub(r\"[^a-zA-Zа-яА-ЯіїєґІЇЄҐ.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "tCIV2VHtI0hu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Завантаження та фільтрація даних\n",
        "MAX_LENGTH = 15  # Максимальна довжина\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"ukr.txt\")\n",
        "    if not os.path.exists('ukr.txt'):\n",
        "        print(\"Помилка\")\n",
        "        return None, None, None\n",
        "    lines = open('ukr.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "           len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    if not pairs: return None, None, None\n",
        "\n",
        "    print(f\"Прочитано {len(pairs)} пар речень\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Відфільтровано до {len(pairs)} пар речень\")\n",
        "    print(\"Створення словників.\")\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(f\"Слів у {input_lang.name}: {input_lang.n_words}\")\n",
        "    print(f\"Слів у {output_lang.name}: {output_lang.n_words}\")\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "s1gtkB3wI_lb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель енкодера\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "mMmnttyLJSls"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель декодера, також вирішив використати Attention\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        # Attention\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # Розрахунок коефіцієнтів уваги\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # Застосування уваги до виходів енкодера\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "rjKYKk0cJZ4C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Переведення у тензори для навчання\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in sentence.split(' '):\n",
        "        if word in lang.word2index:\n",
        "            indexes.append(lang.word2index[word])\n",
        "        else:\n",
        "            indexes.append(UNK_token)\n",
        "    return indexes\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n"
      ],
      "metadata": {
        "id": "1Ojf3ubaJpUK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тренування для однієї пари\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    loss = 0\n",
        "    # Прохід через Encoder\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    # Прохід через Decoder\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # Teacher Forcing, 50% шанс використати правильну відповідь як підказку\n",
        "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "6Q8c50KwJypj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Допоміжні функції для логування\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "QNWBdQNYKPlv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Основний цикл навчання\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "    print(f\"Початок тренування: {n_iters} ітерацій.\")\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))"
      ],
      "metadata": {
        "id": "O8manXH-KXNI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестування роботи моделі.\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                word = output_lang.index2word.get(topi.item(), \"<UNK>\")\n",
        "                decoded_words.append(word)\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=5):\n",
        "    print(\"\\nТестова перевірка.\")\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "def save_models(encoder, decoder):\n",
        "    torch.save(encoder.state_dict(), 'encoder.pth')\n",
        "    torch.save(decoder.state_dict(), 'decoder.pth')"
      ],
      "metadata": {
        "id": "m9NOeG0UKgYg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Підготовка даних\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'ukr', reverse=False)\n",
        "if pairs:\n",
        "    # Створення моделі\n",
        "    hidden_size = 256\n",
        "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "    trainIters(encoder1, attn_decoder1, n_iters=75000, print_every=5000, learning_rate=0.001)\n",
        "    # Перевірка результатів\n",
        "    evaluateRandomly(encoder1, attn_decoder1)\n",
        "    # Збереження\n",
        "    save_models(encoder1, attn_decoder1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2ivb7tEKq1m",
        "outputId": "82915e39-bf63-4775-ddb2-afc630097a54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ukr.txt\n",
            "Прочитано 160049 пар речень\n",
            "Відфільтровано до 159274 пар речень\n",
            "Створення словників.\n",
            "Слів у eng: 10170\n",
            "Слів у ukr: 30884\n",
            "Початок тренування: 75000 ітерацій.\n",
            "2m 21s (- 32m 59s) (5000 6%) 4.7644\n",
            "4m 36s (- 29m 57s) (10000 13%) 4.2025\n",
            "6m 50s (- 27m 22s) (15000 20%) 3.9907\n",
            "9m 4s (- 24m 57s) (20000 26%) 3.9377\n",
            "11m 18s (- 22m 37s) (25000 33%) 3.9086\n",
            "13m 32s (- 20m 18s) (30000 40%) 3.9283\n",
            "15m 47s (- 18m 2s) (35000 46%) 3.8698\n",
            "18m 1s (- 15m 46s) (40000 53%) 3.9053\n",
            "20m 15s (- 13m 30s) (45000 60%) 3.8998\n",
            "22m 29s (- 11m 14s) (50000 66%) 3.8740\n",
            "24m 43s (- 8m 59s) (55000 73%) 3.8564\n",
            "26m 57s (- 6m 44s) (60000 80%) 3.8744\n",
            "29m 12s (- 4m 29s) (65000 86%) 3.8615\n",
            "31m 27s (- 2m 14s) (70000 93%) 3.7702\n",
            "33m 41s (- 0m 0s) (75000 100%) 3.8106\n",
            "\n",
            "Тестова перевірка.\n",
            "> i relied on tom .\n",
            "= я довірилася тому .\n",
            "< я тома тома . <EOS>\n",
            "\n",
            "> tom thought mary could win .\n",
            "= том думав що мері може виграти .\n",
            "< том думав що мері мері . <EOS>\n",
            "\n",
            "> any student can answer that question .\n",
            "= будь який студент може дати відповідь на це запитання .\n",
            "< запитання може це запитання п . <EOS>\n",
            "\n",
            "> do you really think tom is ready ?\n",
            "= ти дійсно думаєш що том готовий ?\n",
            "< ви насправді що що том том ? <EOS>\n",
            "\n",
            "> the toilet is backed up .\n",
            "= унітаз забився .\n",
            "< . <EOS>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}